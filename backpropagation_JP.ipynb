{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rBpSCYEVGM9W",
        "q55lDkQMCWpF",
        "xvu_TbEkeXp4",
        "8asIEhjveJZT",
        "hvIx2APZCTpm",
        "R2U3xEEzCECU",
        "Ig5OyzMjsqAt",
        "Py6H4jlkGKox",
        "IytorQtOCdyP",
        "_-y3eiLN2v-Z"
      ],
      "authorship_tag": "ABX9TyM/sv3Pbg9YsoEq+HuHNfiu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jpscard/backpropagation_tarefa_3/blob/main/backpropagation_JP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CÃ³digo de exemplo"
      ],
      "metadata": {
        "id": "rBpSCYEVGM9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' Code to study MLP training with backpropagation by tracking the training steps.\n",
        "The neural network is a fully-connected multilayer perceptron with 1 hidden layer\n",
        "and 2 inputs, 2 neurons in the hidden layer and 2 output neurons.\n",
        "Hence, the weigth matrices (also called kernels by Keras) have dimensions:\n",
        "W1 for layer 1 has dimension 2 x 2 and W2 of layer 2 has dimension 2 x 2.\n",
        "Recall that when implementing the equations, we use the transposes. For instance,\n",
        "the net output of layer 1 is W1^T x, where x is a column vector of dimension 2 x 1.\n",
        "All neurons have a bias weight.\n",
        "\n",
        "History:\n",
        "I created this TF v2 version as follow:\n",
        "1) Obtained a Tensorflow v1 from\n",
        " https://github.com/keras-team/keras/issues/956 and saved it as\n",
        " backpropagation_tf1_example.py. This code was not compatible with TF v2. \n",
        "2) Then I used\n",
        "tf_upgrade_v2 --infile backpropagation_tf1_example.py --outfile backpropagation_tf2_example.py\n",
        "to create a TF v2 version.\n",
        "3) According to https://stackoverflow.com/questions/66221788/tf-gradients-is-not-supported-when-eager-execution-is-enabled-use-tf-gradientta\n",
        "I added the line:\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "and the code executed properly.\n",
        "4) I made several modifications to initialize the weights with known values\n",
        "and help tracking the training procedure.\n",
        "\n",
        "Aldebaro, UFPA, Jan 2022.\n",
        "'''\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras import backend as k\n",
        "from keras import losses\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#Initialize all weights to match the values adopted in the example at\n",
        "#https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/ \n",
        "#Initialise kernel (weights matrix) to required value\n",
        "def kernel_init_layer1(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (2, 2) \n",
        "    # and create a fixed array with this dimension for layer 1\n",
        "    kernel = np.array([[0.15, 0.20],[0.25, 0.30]])\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (weights matrix) to required value\n",
        "def kernel_init_layer2(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (2, 2)\n",
        "    # and create a fixed array with this dimension for layer 2\n",
        "    kernel = np.array([[0.40, 0.45], [0.50, 0.55]])\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (bias vector) to required value\n",
        "def kernel_init_bias1(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (1, 2)\n",
        "    # and create a fixed array with this dimension for the \n",
        "    # bias vector of layer 1\n",
        "    kernel = np.array([0.35, 0.35]) #both neurons with the same value\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (bias vector) to required value\n",
        "def kernel_init_bias2(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (1, 2)\n",
        "    # and create a fixed array with this dimension for the \n",
        "    # bias vector of layer 2\n",
        "    kernel = np.array([0.60, 0.60]) #both neurons with the same value\n",
        "    return kernel \n",
        "\n",
        "# from https://stackoverflow.com/questions/66221788/tf-gradients-is-not-supported-when-eager-execution-is-enabled-use-tf-gradientta\n",
        "# TF 2 does not use \"eager\" execution, so disable it:\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "#Define the neural network model with dense layers. Syntax:\n",
        "#https://keras.io/api/layers/core_layers/dense/\n",
        "#The sigmoid activation function in Keras is the standard logistic function 1/(1+exp(-x)).\n",
        "#https://keras.io/api/layers/activations/\n",
        "model = Sequential()\n",
        "model.add(Dense(2, input_dim=2, use_bias=True,  bias_initializer=kernel_init_bias1,\n",
        "        kernel_initializer=kernel_init_layer1, activation='sigmoid'))\n",
        "model.add(Dense(2, use_bias=True,  bias_initializer=kernel_init_bias2, \n",
        "        kernel_initializer=kernel_init_layer2, activation='sigmoid'))\n",
        "model.summary() # display the architecture\n",
        "\n",
        "# We are not going to use a Keras optimizer. Define a learning rate:\n",
        "learning_rate = 1  #you can change to 0.5 or any other reasonable value\n",
        "\n",
        "#now compile the model, informing loss and performanc metrics that\n",
        "#should be computed along the training\n",
        "model.compile(loss='mse', metrics=['accuracy'])\n",
        "\n",
        "# Begin TensorFlow\n",
        "sess = tf.compat.v1.InteractiveSession()\n",
        "sess.run(tf.compat.v1.initialize_all_variables())\n",
        "\n",
        "model.optimizer.lr = learning_rate\n",
        "\n",
        "print(\"===START TRAINING THE NETWORK:===\")\n",
        "steps = 3 # steps of gradient descent\n",
        "for s in range(steps):\n",
        "    print('Learning rate = ', k.get_value(model.optimizer.lr))\n",
        "    print(\" ############ Step = \" + str(s) + \" ############\")\n",
        "    print(\"1) ===BEFORE using any GRADIENT:===\")\n",
        "    #define input and target vectors, and also inform it to the loss function object\n",
        "    #in this code we will keep the same inputs and targets along the steps, but the inputs and\n",
        "    #targets can change when we go, for instance, through the training set\n",
        "    inputs = np.array([[0.05, 0.10]]) #define the input for the current iteration (step)\n",
        "    outputs = model.predict(inputs) #forward pass\n",
        "    #define the target vector for the current iteration (step)\n",
        "    targets = np.array([[0.01, 0.99]]) #notice that a sigmoid can output within range [0, 1]\n",
        "    mse = mean_squared_error(targets, outputs) #calculate MSE\n",
        "    #initialize loss object to be later incorporated to the gradients object that\n",
        "    #enables the calculation of the symbolic gradients\n",
        "    loss = losses.mean_squared_error(targets, model.output)\n",
        "    #  ===== Obtain symbolic gradient to calculate numerical gradients =====\n",
        "    gradients = k.gradients(loss, model.trainable_weights) #inform loss and weights\n",
        "    if False: #enable with True in case you want to see the objects\n",
        "        print(\"List of tensors representing the symbolic gradients:\")\n",
        "        for i in range(len(gradients)):\n",
        "            print('symbolic gradient[',i,']=',gradients[i])\n",
        "    print('Network input [x1, x2]:\\n', inputs)\n",
        "    print('Network output [out_o1, out_o2]:\\n', outputs)\n",
        "    print(\"targets:\\n\", targets)\n",
        "    #show weights at the beginning of iteration s\n",
        "    print('weights at the beginning of this step')\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        #note that model.trainable_weights[i] is an object of\n",
        "        # <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable\n",
        "        #therefore (see e.g. https://stackoverflow.com/questions/33679382/how-do-i-get-the-current-value-of-a-variable )\n",
        "        #you need to obtain its value via a TF session:\n",
        "        print('weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "    print(\"MSE with the initial weights:\", mse)\n",
        "\n",
        "    print(\"2) ===After applying GRADIENT:===\")\n",
        "    print(\"------------- Results for step (iteration) =\", s)\n",
        "    # ===== Calculate numerical gradient from symbolic gradients =====\n",
        "    # evaluated_gradients is a list, show its contents:\n",
        "    evaluated_gradients = sess.run(gradients, feed_dict={model.input: inputs})\n",
        "    print('Gradients g to be used in new_weights = current_weights - learning_rate*g')\n",
        "    for i in range(len(evaluated_gradients)):\n",
        "        print('gradients[',i,']=',evaluated_gradients[i])\n",
        "\n",
        "    # Apply (\"step down\") the gradient for each layer, subtracting the gradients\n",
        "    # from current weights scaled by the learning rate:\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        sess.run(tf.compat.v1.assign_sub(model.trainable_weights[i], learning_rate*evaluated_gradients[i]))\n",
        "\n",
        "    #show weights after gradient propagation of iteration s\n",
        "    print('weights after gradient propagation in this step')\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        print('weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "\n",
        "    # print the MSE with new weights:\n",
        "    outputs = model.predict(inputs)\n",
        "    mse = mean_squared_error(targets, outputs)\n",
        "    print(\"MSE with the new weights:\", mse)\n",
        "\n",
        "#Collect and show final results\n",
        "final_outputs = model.predict(inputs)\n",
        "final_mse = mean_squared_error(targets, final_outputs)\n",
        "\n",
        "print(\"\\n ===AFTER executing all GRADIENT descent steps===\")\n",
        "#show weights at the end of training\n",
        "for i in range(len(model.trainable_weights)):\n",
        "    #note that model.trainable_weights[i] is an object of\n",
        "    # <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable\n",
        "    #therefore (see e.g. https://stackoverflow.com/questions/33679382/how-do-i-get-the-current-value-of-a-variable )\n",
        "    #you need to obtain its value via a TF session:\n",
        "    print('final weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "print(\"outputs:\\n\", final_outputs)\n",
        "print(\"targets:\\n\", targets)\n",
        "print(\"Final MSE = \", final_mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJedYdyLBBnw",
        "outputId": "f0b4bfdc-c28c-4e5d-c7e2-8c862256dafd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 2)                 6         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 6         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12\n",
            "Trainable params: 12\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/tf_should_use.py:243: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===START TRAINING THE NETWORK:===\n",
            "Learning rate =  1\n",
            " ############ Step = 0 ############\n",
            "1) ===BEFORE using any GRADIENT:===\n",
            "Network input [x1, x2]:\n",
            " [[0.05 0.1 ]]\n",
            "Network output [out_o1, out_o2]:\n",
            " [[0.7569319 0.7677179]]\n",
            "targets:\n",
            " [[0.01 0.99]]\n",
            "weights at the beginning of this step\n",
            "weights[ 0 ]= [[0.15 0.2 ]\n",
            " [0.25 0.3 ]]\n",
            "weights[ 1 ]= [0.35 0.35]\n",
            "weights[ 2 ]= [[0.4  0.45]\n",
            " [0.5  0.55]]\n",
            "weights[ 3 ]= [0.6 0.6]\n",
            "MSE with the initial weights: 0.3036582988081465\n",
            "2) ===After applying GRADIENT:===\n",
            "------------- Results for step (iteration) = 0\n",
            "Gradients g to be used in new_weights = current_weights - learning_rate*g\n",
            "gradients[ 0 ]= [[0.00044758 0.00056464]\n",
            " [0.00089517 0.00112929]]\n",
            "gradients[ 1 ]= [0.00895169 0.01129289]\n",
            "gradients[ 2 ]= [[ 0.08169585 -0.02356439]\n",
            " [ 0.08194415 -0.02363601]]\n",
            "gradients[ 3 ]= [ 0.137425   -0.03963893]\n",
            "weights after gradient propagation in this step\n",
            "weights[ 0 ]= [[0.14955242 0.19943535]\n",
            " [0.24910483 0.2988707 ]]\n",
            "weights[ 1 ]= [0.3410483 0.3387071]\n",
            "weights[ 2 ]= [[0.31830415 0.4735644 ]\n",
            " [0.41805586 0.573636  ]]\n",
            "weights[ 3 ]= [0.46257502 0.63963896]\n",
            "MSE with the new weights: 0.2677876667422917\n",
            "Learning rate =  1\n",
            " ############ Step = 1 ############\n",
            "1) ===BEFORE using any GRADIENT:===\n",
            "Network input [x1, x2]:\n",
            " [[0.05 0.1 ]]\n",
            "Network output [out_o1, out_o2]:\n",
            " [[0.71079135 0.77912843]]\n",
            "targets:\n",
            " [[0.01 0.99]]\n",
            "weights at the beginning of this step\n",
            "weights[ 0 ]= [[0.14955242 0.19943535]\n",
            " [0.24910483 0.2988707 ]]\n",
            "weights[ 1 ]= [0.3410483 0.3387071]\n",
            "weights[ 2 ]= [[0.31830415 0.4735644 ]\n",
            " [0.41805586 0.573636  ]]\n",
            "weights[ 3 ]= [0.46257502 0.63963896]\n",
            "MSE with the initial weights: 0.2677876667422917\n",
            "2) ===After applying GRADIENT:===\n",
            "------------- Results for step (iteration) = 1\n",
            "Gradients g to be used in new_weights = current_weights - learning_rate*g\n",
            "gradients[ 0 ]= [[0.00034616 0.00047537]\n",
            " [0.00069233 0.00095074]]\n",
            "gradients[ 1 ]= [0.00692329 0.00950745]\n",
            "gradients[ 2 ]= [[ 0.08532492 -0.02149318]\n",
            " [ 0.08550328 -0.02153811]]\n",
            "gradients[ 3 ]= [ 0.14405958 -0.03628832]\n",
            "weights after gradient propagation in this step\n",
            "weights[ 0 ]= [[0.14920625 0.19895998]\n",
            " [0.2484125  0.29791996]]\n",
            "weights[ 1 ]= [0.334125   0.32919964]\n",
            "weights[ 2 ]= [[0.23297924 0.49505755]\n",
            " [0.33255258 0.5951741 ]]\n",
            "weights[ 3 ]= [0.31851542 0.6759273 ]\n",
            "MSE with the new weights: 0.22986570303617748\n",
            "Learning rate =  1\n",
            " ############ Step = 2 ############\n",
            "1) ===BEFORE using any GRADIENT:===\n",
            "Network input [x1, x2]:\n",
            " [[0.05 0.1 ]]\n",
            "Network output [out_o1, out_o2]:\n",
            " [[0.65762264 0.78921074]]\n",
            "targets:\n",
            " [[0.01 0.99]]\n",
            "weights at the beginning of this step\n",
            "weights[ 0 ]= [[0.14920625 0.19895998]\n",
            " [0.2484125  0.29791996]]\n",
            "weights[ 1 ]= [0.334125   0.32919964]\n",
            "weights[ 2 ]= [[0.23297924 0.49505755]\n",
            " [0.33255258 0.5951741 ]]\n",
            "weights[ 3 ]= [0.31851542 0.6759273 ]\n",
            "MSE with the initial weights: 0.22986570303617748\n",
            "2) ===After applying GRADIENT:===\n",
            "------------- Results for step (iteration) = 2\n",
            "Gradients g to be used in new_weights = current_weights - learning_rate*g\n",
            "gradients[ 0 ]= [[0.00021079 0.00034574]\n",
            " [0.00042158 0.00069147]]\n",
            "gradients[ 1 ]= [0.00421583 0.00691474]\n",
            "gradients[ 2 ]= [[ 0.08611797 -0.01972749]\n",
            " [ 0.08620656 -0.01974778]]\n",
            "gradients[ 3 ]= [ 0.14581555 -0.03340273]\n",
            "weights after gradient propagation in this step\n",
            "weights[ 0 ]= [[0.14899546 0.19861424]\n",
            " [0.24799092 0.2972285 ]]\n",
            "weights[ 1 ]= [0.32990918 0.3222849 ]\n",
            "weights[ 2 ]= [[0.14686127 0.51478505]\n",
            " [0.24634603 0.61492187]]\n",
            "weights[ 3 ]= [0.17269987 0.70933   ]\n",
            "MSE with the new weights: 0.19230032630125046\n",
            "\n",
            " ===AFTER executing all GRADIENT descent steps===\n",
            "final weights[ 0 ]= [[0.14899546 0.19861424]\n",
            " [0.24799092 0.2972285 ]]\n",
            "final weights[ 1 ]= [0.32990918 0.3222849 ]\n",
            "final weights[ 2 ]= [[0.14686127 0.51478505]\n",
            " [0.24634603 0.61492187]]\n",
            "final weights[ 3 ]= [0.17269987 0.70933   ]\n",
            "outputs:\n",
            " [[0.59977007 0.79823995]]\n",
            "targets:\n",
            " [[0.01 0.99]]\n",
            "Final MSE =  0.19230032630125046\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Codigo de aplicaÃ§Ã£o para a amostra_1 (1.2,0,1,-1.3) "
      ],
      "metadata": {
        "id": "q55lDkQMCWpF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Para Learning rate = 1 "
      ],
      "metadata": {
        "id": "xvu_TbEkeXp4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e24ae87-ec5b-4da6-ee0b-f3a90f4798cf",
        "id": "FTSeoo9-CWpG"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_8 (Dense)             (None, 2)                 6         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 2)                 6         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12\n",
            "Trainable params: 12\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "===START TRAINING THE NETWORK:===\n",
            "Learning rate =  1\n",
            " ############ Step = 0 ############\n",
            "1) ===BEFORE using any GRADIENT:===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py:1768: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network input [x1, x2]:\n",
            " [[1.2 0. ]]\n",
            "Network output [out_o1, out_o2]:\n",
            " [[0.6609764 0.6940291]]\n",
            "targets:\n",
            " [[ 1.  -1.3]]\n",
            "weights at the beginning of this step\n",
            "weights[ 0 ]= [[ 0.1  0.3]\n",
            " [-0.4  0.2]]\n",
            "weights[ 1 ]= [-0.4  0.8]\n",
            "weights[ 2 ]= [[ 0.4  0.2]\n",
            " [-0.4  0.7]]\n",
            "weights[ 3 ]= [0.8 0.2]\n",
            "MSE with the initial weights: 2.0455445087618966\n",
            "2) ===After applying GRADIENT:===\n",
            "------------- Results for step (iteration) = 0\n",
            "Gradients g to be used in new_weights = current_weights - learning_rate*g\n",
            "gradients[ 0 ]= [[0.01597462 0.07125631]\n",
            " [0.         0.        ]]\n",
            "gradients[ 1 ]= [0.01331218 0.05938026]\n",
            "gradients[ 2 ]= [[-0.03270185  0.18227024]\n",
            " [-0.05783894  0.3223768 ]]\n",
            "gradients[ 3 ]= [-0.07597064  0.42343745]\n",
            "weights after gradient propagation in this step\n",
            "weights[ 0 ]= [[ 0.08402538  0.2287437 ]\n",
            " [-0.4         0.2       ]]\n",
            "weights[ 1 ]= [-0.4133122  0.7406198]\n",
            "weights[ 2 ]= [[ 0.43270186  0.01772976]\n",
            " [-0.34216106  0.3776232 ]]\n",
            "weights[ 3 ]= [ 0.87597066 -0.22343744]\n",
            "MSE with the new weights: 1.6952182074434803\n",
            "Learning rate =  1\n",
            " ############ Step = 1 ############\n",
            "1) ===BEFORE using any GRADIENT:===\n",
            "Network input [x1, x2]:\n",
            " [[1.2 0. ]]\n",
            "Network output [out_o1, out_o2]:\n",
            " [[0.69160604 0.51530427]]\n",
            "targets:\n",
            " [[ 1.  -1.3]]\n",
            "weights at the beginning of this step\n",
            "weights[ 0 ]= [[ 0.08402538  0.2287437 ]\n",
            " [-0.4         0.2       ]]\n",
            "weights[ 1 ]= [-0.4133122  0.7406198]\n",
            "weights[ 2 ]= [[ 0.43270186  0.01772976]\n",
            " [-0.34216106  0.3776232 ]]\n",
            "weights[ 3 ]= [ 0.87597066 -0.22343744]\n",
            "MSE with the initial weights: 1.6952182074434803\n",
            "2) ===After applying GRADIENT:===\n",
            "------------- Results for step (iteration) = 1\n",
            "Gradients g to be used in new_weights = current_weights - learning_rate*g\n",
            "gradients[ 0 ]= [[-0.00597971  0.04538529]\n",
            " [-0.          0.        ]]\n",
            "gradients[ 1 ]= [-0.00498309  0.03782107]\n",
            "gradients[ 2 ]= [[-0.02779115  0.191566  ]\n",
            " [-0.0482812   0.33280504]]\n",
            "gradients[ 3 ]= [-0.06577646  0.45340088]\n",
            "weights after gradient propagation in this step\n",
            "weights[ 0 ]= [[ 0.09000509  0.18335842]\n",
            " [-0.4         0.2       ]]\n",
            "weights[ 1 ]= [-0.4083291  0.7027987]\n",
            "weights[ 2 ]= [[ 0.460493   -0.17383625]\n",
            " [-0.29387987  0.04481816]]\n",
            "weights[ 3 ]= [ 0.9417471  -0.67683834]\n",
            "MSE with the new weights: 1.3648338513786415\n",
            "Learning rate =  1\n",
            " ############ Step = 2 ############\n",
            "1) ===BEFORE using any GRADIENT:===\n",
            "Network input [x1, x2]:\n",
            " [[1.2 0. ]]\n",
            "Network output [out_o1, out_o2]:\n",
            " [[0.71654165 0.3276729 ]]\n",
            "targets:\n",
            " [[ 1.  -1.3]]\n",
            "weights at the beginning of this step\n",
            "weights[ 0 ]= [[ 0.09000509  0.18335842]\n",
            " [-0.4         0.2       ]]\n",
            "weights[ 1 ]= [-0.4083291  0.7027987]\n",
            "weights[ 2 ]= [[ 0.460493   -0.17383625]\n",
            " [-0.29387987  0.04481816]]\n",
            "weights[ 3 ]= [ 0.9417471  -0.67683834]\n",
            "MSE with the initial weights: 1.3648338513786415\n",
            "2) ===After applying GRADIENT:===\n",
            "------------- Results for step (iteration) = 2\n",
            "Gradients g to be used in new_weights = current_weights - learning_rate*g\n",
            "gradients[ 0 ]= [[-0.02606188  0.00805665]\n",
            " [-0.          0.        ]]\n",
            "gradients[ 1 ]= [-0.02171823  0.00671387]\n",
            "gradients[ 2 ]= [[-0.02449614  0.15256888]\n",
            " [-0.04120038  0.25660768]]\n",
            "gradients[ 3 ]= [-0.05757314  0.35858184]\n",
            "weights after gradient propagation in this step\n",
            "weights[ 0 ]= [[ 0.11606697  0.17530178]\n",
            " [-0.4         0.2       ]]\n",
            "weights[ 1 ]= [-0.38661087  0.69608486]\n",
            "weights[ 2 ]= [[ 0.48498914 -0.3264051 ]\n",
            " [-0.25267947 -0.21178952]]\n",
            "weights[ 3 ]= [ 0.99932027 -1.0354202 ]\n",
            "MSE with the new weights: 1.1734368570975322\n",
            "\n",
            " ===AFTER executing all GRADIENT descent steps===\n",
            "final weights[ 0 ]= [[ 0.11606697  0.17530178]\n",
            " [-0.4         0.2       ]]\n",
            "final weights[ 1 ]= [-0.38661087  0.69608486]\n",
            "final weights[ 2 ]= [[ 0.48498914 -0.3264051 ]\n",
            " [-0.25267947 -0.21178952]]\n",
            "final weights[ 3 ]= [ 0.99932027 -1.0354202 ]\n",
            "outputs:\n",
            " [[0.73730385 0.20925957]]\n",
            "targets:\n",
            " [[ 1.  -1.3]]\n",
            "Final MSE =  1.1734368570975322\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras import backend as k\n",
        "from keras import losses\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#Initialize all weights to match the values adopted in the example at\n",
        "#https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/ \n",
        "#Initialise kernel (weights matrix) to required value\n",
        "def kernel_init_layer1(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (2, 2) \n",
        "    # and create a fixed array with this dimension for layer 1\n",
        "    kernel = np.array([[0.1, 0.3],[-0.4, 0.2]])\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (weights matrix) to required value\n",
        "def kernel_init_layer2(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (2, 2)\n",
        "    # and create a fixed array with this dimension for layer 2\n",
        "    kernel = np.array([[0.4, 0.2], [-0.4, 0.7]])\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (bias vector) to required value\n",
        "def kernel_init_bias1(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (1, 2)\n",
        "    # and create a fixed array with this dimension for the \n",
        "    # bias vector of layer 1\n",
        "    kernel = np.array([-0.4, 0.8]) #both neurons with the same value\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (bias vector) to required value\n",
        "def kernel_init_bias2(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (1, 2)\n",
        "    # and create a fixed array with this dimension for the \n",
        "    # bias vector of layer 2\n",
        "    kernel = np.array([0.8, 0.2]) #both neurons with the same value\n",
        "    return kernel \n",
        "\n",
        "# from https://stackoverflow.com/questions/66221788/tf-gradients-is-not-supported-when-eager-execution-is-enabled-use-tf-gradientta\n",
        "# TF 2 does not use \"eager\" execution, so disable it:\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "#Define the neural network model with dense layers. Syntax:\n",
        "#https://keras.io/api/layers/core_layers/dense/\n",
        "#The sigmoid activation function in Keras is the standard logistic function 1/(1+exp(-x)).\n",
        "#https://keras.io/api/layers/activations/\n",
        "model = Sequential()\n",
        "model.add(Dense(2, input_dim=2, use_bias=True,  bias_initializer=kernel_init_bias1,\n",
        "        kernel_initializer=kernel_init_layer1, activation='sigmoid'))\n",
        "model.add(Dense(2, use_bias=True,  bias_initializer=kernel_init_bias2, \n",
        "        kernel_initializer=kernel_init_layer2, activation='sigmoid'))\n",
        "model.summary() # display the architecture\n",
        "\n",
        "# We are not going to use a Keras optimizer. Define a learning rate:\n",
        "learning_rate = 1  #you can change to 0.5 or any other reasonable value\n",
        "\n",
        "#now compile the model, informing loss and performanc metrics that\n",
        "#should be computed along the training\n",
        "model.compile(loss='mse', metrics=['accuracy'])\n",
        "\n",
        "# Begin TensorFlow\n",
        "sess = tf.compat.v1.InteractiveSession()\n",
        "sess.run(tf.compat.v1.initialize_all_variables())\n",
        "\n",
        "model.optimizer.lr = learning_rate\n",
        "\n",
        "print(\"===START TRAINING THE NETWORK:===\")\n",
        "steps = 3 # steps of gradient descent\n",
        "for s in range(steps):\n",
        "    print('Learning rate = ', k.get_value(model.optimizer.lr))\n",
        "    print(\" ############ Step = \" + str(s) + \" ############\")\n",
        "    print(\"1) ===BEFORE using any GRADIENT:===\")\n",
        "    #define input and target vectors, and also inform it to the loss function object\n",
        "    #in this code we will keep the same inputs and targets along the steps, but the inputs and\n",
        "    #targets can change when we go, for instance, through the training set\n",
        "    inputs = np.array([[1.2,0]]) #define the input for the current iteration (step)\n",
        "    outputs = model.predict(inputs) #forward pass\n",
        "    #define the target vector for the current iteration (step)\n",
        "    targets = np.array([[1, -1.3]]) #notice that a sigmoid can output within range [0, 1]\n",
        "    mse = mean_squared_error(targets, outputs) #calculate MSE\n",
        "    #initialize loss object to be later incorporated to the gradients object that\n",
        "    #enables the calculation of the symbolic gradients\n",
        "    loss = losses.mean_squared_error(targets, model.output)\n",
        "    #  ===== Obtain symbolic gradient to calculate numerical gradients =====\n",
        "    gradients = k.gradients(loss, model.trainable_weights) #inform loss and weights\n",
        "    if False: #enable with True in case you want to see the objects\n",
        "        print(\"List of tensors representing the symbolic gradients:\")\n",
        "        for i in range(len(gradients)):\n",
        "            print('symbolic gradient[',i,']=',gradients[i])\n",
        "    print('Network input [x1, x2]:\\n', inputs)\n",
        "    print('Network output [out_o1, out_o2]:\\n', outputs)\n",
        "    print(\"targets:\\n\", targets)\n",
        "    #show weights at the beginning of iteration s\n",
        "    print('weights at the beginning of this step')\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        #note that model.trainable_weights[i] is an object of\n",
        "        # <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable\n",
        "        #therefore (see e.g. https://stackoverflow.com/questions/33679382/how-do-i-get-the-current-value-of-a-variable )\n",
        "        #you need to obtain its value via a TF session:\n",
        "        print('weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "    print(\"MSE with the initial weights:\", mse)\n",
        "\n",
        "    print(\"2) ===After applying GRADIENT:===\")\n",
        "    print(\"------------- Results for step (iteration) =\", s)\n",
        "    # ===== Calculate numerical gradient from symbolic gradients =====\n",
        "    # evaluated_gradients is a list, show its contents:\n",
        "    evaluated_gradients = sess.run(gradients, feed_dict={model.input: inputs})\n",
        "    print('Gradients g to be used in new_weights = current_weights - learning_rate*g')\n",
        "    for i in range(len(evaluated_gradients)):\n",
        "        print('gradients[',i,']=',evaluated_gradients[i])\n",
        "\n",
        "    # Apply (\"step down\") the gradient for each layer, subtracting the gradients\n",
        "    # from current weights scaled by the learning rate:\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        sess.run(tf.compat.v1.assign_sub(model.trainable_weights[i], learning_rate*evaluated_gradients[i]))\n",
        "\n",
        "    #show weights after gradient propagation of iteration s\n",
        "    print('weights after gradient propagation in this step')\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        print('weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "\n",
        "    # print the MSE with new weights:\n",
        "    outputs = model.predict(inputs)\n",
        "    mse = mean_squared_error(targets, outputs)\n",
        "    print(\"MSE with the new weights:\", mse)\n",
        "\n",
        "#Collect and show final results\n",
        "final_outputs = model.predict(inputs)\n",
        "final_mse = mean_squared_error(targets, final_outputs)\n",
        "\n",
        "print(\"\\n ===AFTER executing all GRADIENT descent steps===\")\n",
        "#show weights at the end of training\n",
        "for i in range(len(model.trainable_weights)):\n",
        "    #note that model.trainable_weights[i] is an object of\n",
        "    # <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable\n",
        "    #therefore (see e.g. https://stackoverflow.com/questions/33679382/how-do-i-get-the-current-value-of-a-variable )\n",
        "    #you need to obtain its value via a TF session:\n",
        "    print('final weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "print(\"outputs:\\n\", final_outputs)\n",
        "print(\"targets:\\n\", targets)\n",
        "print(\"Final MSE = \", final_mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Para Learning rate = 0,5 "
      ],
      "metadata": {
        "id": "8asIEhjveJZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras import backend as k\n",
        "from keras import losses\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#Initialize all weights to match the values adopted in the example at\n",
        "#https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/ \n",
        "#Initialise kernel (weights matrix) to required value\n",
        "def kernel_init_layer1(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (2, 2) \n",
        "    # and create a fixed array with this dimension for layer 1\n",
        "    kernel = np.array([[0.1, 0.3],[-0.4, 0.2]])\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (weights matrix) to required value\n",
        "def kernel_init_layer2(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (2, 2)\n",
        "    # and create a fixed array with this dimension for layer 2\n",
        "    kernel = np.array([[0.4, 0.2], [-0.4, 0.7]])\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (bias vector) to required value\n",
        "def kernel_init_bias1(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (1, 2)\n",
        "    # and create a fixed array with this dimension for the \n",
        "    # bias vector of layer 1\n",
        "    kernel = np.array([-0.4, 0.8]) #both neurons with the same value\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (bias vector) to required value\n",
        "def kernel_init_bias2(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (1, 2)\n",
        "    # and create a fixed array with this dimension for the \n",
        "    # bias vector of layer 2\n",
        "    kernel = np.array([0.8, 0.2]) #both neurons with the same value\n",
        "    return kernel \n",
        "\n",
        "# from https://stackoverflow.com/questions/66221788/tf-gradients-is-not-supported-when-eager-execution-is-enabled-use-tf-gradientta\n",
        "# TF 2 does not use \"eager\" execution, so disable it:\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "#Define the neural network model with dense layers. Syntax:\n",
        "#https://keras.io/api/layers/core_layers/dense/\n",
        "#The sigmoid activation function in Keras is the standard logistic function 1/(1+exp(-x)).\n",
        "#https://keras.io/api/layers/activations/\n",
        "model = Sequential()\n",
        "model.add(Dense(2, input_dim=2, use_bias=True,  bias_initializer=kernel_init_bias1,\n",
        "        kernel_initializer=kernel_init_layer1, activation='sigmoid'))\n",
        "model.add(Dense(2, use_bias=True,  bias_initializer=kernel_init_bias2, \n",
        "        kernel_initializer=kernel_init_layer2, activation='sigmoid'))\n",
        "model.summary() # display the architecture\n",
        "\n",
        "# We are not going to use a Keras optimizer. Define a learning rate:\n",
        "learning_rate = 0.5  #you can change to 0.5 or any other reasonable value\n",
        "\n",
        "#now compile the model, informing loss and performanc metrics that\n",
        "#should be computed along the training\n",
        "model.compile(loss='mse', metrics=['accuracy'])\n",
        "\n",
        "# Begin TensorFlow\n",
        "sess = tf.compat.v1.InteractiveSession()\n",
        "sess.run(tf.compat.v1.initialize_all_variables())\n",
        "\n",
        "model.optimizer.lr = learning_rate\n",
        "\n",
        "print(\"===START TRAINING THE NETWORK:===\")\n",
        "steps = 3 # steps of gradient descent\n",
        "for s in range(steps):\n",
        "    print('Learning rate = ', k.get_value(model.optimizer.lr))\n",
        "    print(\" ############ Step = \" + str(s) + \" ############\")\n",
        "    print(\"1) ===BEFORE using any GRADIENT:===\")\n",
        "    #define input and target vectors, and also inform it to the loss function object\n",
        "    #in this code we will keep the same inputs and targets along the steps, but the inputs and\n",
        "    #targets can change when we go, for instance, through the training set\n",
        "    inputs = np.array([[1.2,0]]) #define the input for the current iteration (step)\n",
        "    outputs = model.predict(inputs) #forward pass\n",
        "    #define the target vector for the current iteration (step)\n",
        "    targets = np.array([[1, -1.3]]) #notice that a sigmoid can output within range [0, 1]\n",
        "    mse = mean_squared_error(targets, outputs) #calculate MSE\n",
        "    #initialize loss object to be later incorporated to the gradients object that\n",
        "    #enables the calculation of the symbolic gradients\n",
        "    loss = losses.mean_squared_error(targets, model.output)\n",
        "    #  ===== Obtain symbolic gradient to calculate numerical gradients =====\n",
        "    gradients = k.gradients(loss, model.trainable_weights) #inform loss and weights\n",
        "    if False: #enable with True in case you want to see the objects\n",
        "        print(\"List of tensors representing the symbolic gradients:\")\n",
        "        for i in range(len(gradients)):\n",
        "            print('symbolic gradient[',i,']=',gradients[i])\n",
        "    print('Network input [x1, x2]:\\n', inputs)\n",
        "    print('Network output [out_o1, out_o2]:\\n', outputs)\n",
        "    print(\"targets:\\n\", targets)\n",
        "    #show weights at the beginning of iteration s\n",
        "    print('weights at the beginning of this step')\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        #note that model.trainable_weights[i] is an object of\n",
        "        # <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable\n",
        "        #therefore (see e.g. https://stackoverflow.com/questions/33679382/how-do-i-get-the-current-value-of-a-variable )\n",
        "        #you need to obtain its value via a TF session:\n",
        "        print('weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "    print(\"MSE with the initial weights:\", mse)\n",
        "\n",
        "    print(\"2) ===After applying GRADIENT:===\")\n",
        "    print(\"------------- Results for step (iteration) =\", s)\n",
        "    # ===== Calculate numerical gradient from symbolic gradients =====\n",
        "    # evaluated_gradients is a list, show its contents:\n",
        "    evaluated_gradients = sess.run(gradients, feed_dict={model.input: inputs})\n",
        "    print('Gradients g to be used in new_weights = current_weights - learning_rate*g')\n",
        "    for i in range(len(evaluated_gradients)):\n",
        "        print('gradients[',i,']=',evaluated_gradients[i])\n",
        "\n",
        "    # Apply (\"step down\") the gradient for each layer, subtracting the gradients\n",
        "    # from current weights scaled by the learning rate:\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        sess.run(tf.compat.v1.assign_sub(model.trainable_weights[i], learning_rate*evaluated_gradients[i]))\n",
        "\n",
        "    #show weights after gradient propagation of iteration s\n",
        "    print('weights after gradient propagation in this step')\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        print('weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "\n",
        "    # print the MSE with new weights:\n",
        "    outputs = model.predict(inputs)\n",
        "    mse = mean_squared_error(targets, outputs)\n",
        "    print(\"MSE with the new weights:\", mse)\n",
        "\n",
        "#Collect and show final results\n",
        "final_outputs = model.predict(inputs)\n",
        "final_mse = mean_squared_error(targets, final_outputs)\n",
        "\n",
        "print(\"\\n ===AFTER executing all GRADIENT descent steps===\")\n",
        "#show weights at the end of training\n",
        "for i in range(len(model.trainable_weights)):\n",
        "    #note that model.trainable_weights[i] is an object of\n",
        "    # <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable\n",
        "    #therefore (see e.g. https://stackoverflow.com/questions/33679382/how-do-i-get-the-current-value-of-a-variable )\n",
        "    #you need to obtain its value via a TF session:\n",
        "    print('final weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "print(\"outputs:\\n\", final_outputs)\n",
        "print(\"targets:\\n\", targets)\n",
        "print(\"Final MSE = \", final_mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "te2TSvMGd-_a",
        "outputId": "ccc46cdb-c8fa-4c33-e220-7491ac1cfd57"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_10 (Dense)            (None, 2)                 6         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 2)                 6         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12\n",
            "Trainable params: 12\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "===START TRAINING THE NETWORK:===\n",
            "Learning rate =  0.5\n",
            " ############ Step = 0 ############\n",
            "1) ===BEFORE using any GRADIENT:===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py:1768: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network input [x1, x2]:\n",
            " [[1.2 0. ]]\n",
            "Network output [out_o1, out_o2]:\n",
            " [[0.6609764 0.6940291]]\n",
            "targets:\n",
            " [[ 1.  -1.3]]\n",
            "weights at the beginning of this step\n",
            "weights[ 0 ]= [[ 0.1  0.3]\n",
            " [-0.4  0.2]]\n",
            "weights[ 1 ]= [-0.4  0.8]\n",
            "weights[ 2 ]= [[ 0.4  0.2]\n",
            " [-0.4  0.7]]\n",
            "weights[ 3 ]= [0.8 0.2]\n",
            "MSE with the initial weights: 2.0455445087618966\n",
            "2) ===After applying GRADIENT:===\n",
            "------------- Results for step (iteration) = 0\n",
            "Gradients g to be used in new_weights = current_weights - learning_rate*g\n",
            "gradients[ 0 ]= [[0.01597462 0.07125631]\n",
            " [0.         0.        ]]\n",
            "gradients[ 1 ]= [0.01331218 0.05938026]\n",
            "gradients[ 2 ]= [[-0.03270185  0.18227024]\n",
            " [-0.05783894  0.3223768 ]]\n",
            "gradients[ 3 ]= [-0.07597064  0.42343745]\n",
            "weights after gradient propagation in this step\n",
            "weights[ 0 ]= [[ 0.09201269  0.26437187]\n",
            " [-0.4         0.2       ]]\n",
            "weights[ 1 ]= [-0.4066561   0.77030987]\n",
            "weights[ 2 ]= [[ 0.41635093  0.10886488]\n",
            " [-0.37108055  0.53881156]]\n",
            "weights[ 3 ]= [ 0.83798534 -0.01171872]\n",
            "MSE with the new weights: 1.8719860543275522\n",
            "Learning rate =  0.5\n",
            " ############ Step = 1 ############\n",
            "1) ===BEFORE using any GRADIENT:===\n",
            "Network input [x1, x2]:\n",
            " [[1.2 0. ]]\n",
            "Network output [out_o1, out_o2]:\n",
            " [[0.6765606 0.6077104]]\n",
            "targets:\n",
            " [[ 1.  -1.3]]\n",
            "weights at the beginning of this step\n",
            "weights[ 0 ]= [[ 0.09201269  0.26437187]\n",
            " [-0.4         0.2       ]]\n",
            "weights[ 1 ]= [-0.4066561   0.77030987]\n",
            "weights[ 2 ]= [[ 0.41635093  0.10886488]\n",
            " [-0.37108055  0.53881156]]\n",
            "weights[ 3 ]= [ 0.83798534 -0.01171872]\n",
            "MSE with the initial weights: 1.8719860543275522\n",
            "2) ===After applying GRADIENT:===\n",
            "------------- Results for step (iteration) = 1\n",
            "Gradients g to be used in new_weights = current_weights - learning_rate*g\n",
            "gradients[ 0 ]= [[0.00588292 0.0613824 ]\n",
            " [0.         0.        ]]\n",
            "gradients[ 1 ]= [0.00490244 0.051152  ]\n",
            "gradients[ 2 ]= [[-0.03018477  0.19395958]\n",
            " [-0.05293567  0.340151  ]]\n",
            "gradients[ 3 ]= [-0.07077707  0.45479524]\n",
            "weights after gradient propagation in this step\n",
            "weights[ 0 ]= [[ 0.08907123  0.23368067]\n",
            " [-0.4         0.2       ]]\n",
            "weights[ 1 ]= [-0.4091073   0.74473387]\n",
            "weights[ 2 ]= [[ 0.4314433   0.01188509]\n",
            " [-0.34461272  0.36873606]]\n",
            "weights[ 3 ]= [ 0.87337387 -0.23911634]\n",
            "MSE with the new weights: 1.6846850065642938\n",
            "Learning rate =  0.5\n",
            " ############ Step = 2 ############\n",
            "1) ===BEFORE using any GRADIENT:===\n",
            "Network input [x1, x2]:\n",
            " [[1.2 0. ]]\n",
            "Network output [out_o1, out_o2]:\n",
            " [[0.6906409  0.50932777]]\n",
            "targets:\n",
            " [[ 1.  -1.3]]\n",
            "weights at the beginning of this step\n",
            "weights[ 0 ]= [[ 0.08907123  0.23368067]\n",
            " [-0.4         0.2       ]]\n",
            "weights[ 1 ]= [-0.4091073   0.74473387]\n",
            "weights[ 2 ]= [[ 0.4314433   0.01188509]\n",
            " [-0.34461272  0.36873606]]\n",
            "weights[ 3 ]= [ 0.87337387 -0.23911634]\n",
            "MSE with the initial weights: 1.6846850065642938\n",
            "2) ===After applying GRADIENT:===\n",
            "------------- Results for step (iteration) = 2\n",
            "Gradients g to be used in new_weights = current_weights - learning_rate*g\n",
            "gradients[ 0 ]= [[-0.00678667  0.04418995]\n",
            " [-0.          0.        ]]\n",
            "gradients[ 1 ]= [-0.00565555  0.03682496]\n",
            "gradients[ 2 ]= [[-0.02809194  0.1921807 ]\n",
            " [-0.0486453   0.33278897]]\n",
            "gradients[ 3 ]= [-0.06609643  0.4521745 ]\n",
            "weights after gradient propagation in this step\n",
            "weights[ 0 ]= [[ 0.09246456  0.21158569]\n",
            " [-0.4         0.2       ]]\n",
            "weights[ 1 ]= [-0.40627953  0.7263214 ]\n",
            "weights[ 2 ]= [[ 0.4454893  -0.08420525]\n",
            " [-0.32029006  0.20234157]]\n",
            "weights[ 3 ]= [ 0.9064221  -0.46520358]\n",
            "MSE with the new weights: 1.5101597490018819\n",
            "\n",
            " ===AFTER executing all GRADIENT descent steps===\n",
            "final weights[ 0 ]= [[ 0.09246456  0.21158569]\n",
            " [-0.4         0.2       ]]\n",
            "final weights[ 1 ]= [-0.40627953  0.7263214 ]\n",
            "final weights[ 2 ]= [[ 0.4454893  -0.08420525]\n",
            " [-0.32029006  0.20234157]]\n",
            "final weights[ 3 ]= [ 0.9064221  -0.46520358]\n",
            "outputs:\n",
            " [[0.70341504 0.41241258]]\n",
            "targets:\n",
            " [[ 1.  -1.3]]\n",
            "Final MSE =  1.5101597490018819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Codigo de aplicaÃ§Ã£o para a amostra_2 (-1.2,-1,-1,0.9)"
      ],
      "metadata": {
        "id": "hvIx2APZCTpm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Para Learning rate = 1"
      ],
      "metadata": {
        "id": "R2U3xEEzCECU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05a7e0cc-ce3f-4f7e-f277-4e3a7cd7d08e",
        "id": "gQEGKEnVxFnz"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_4 (Dense)             (None, 2)                 6         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 2)                 6         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12\n",
            "Trainable params: 12\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "===START TRAINING THE NETWORK:===\n",
            "Learning rate =  1\n",
            " ############ Step = 0 ############\n",
            "1) ===BEFORE using any GRADIENT:===\n",
            "Network input [x1, x2]:\n",
            " [[-1.2 -1. ]]\n",
            "Network output [out_o1, out_o2]:\n",
            " [[0.6822494  0.66503346]]\n",
            "targets:\n",
            " [[-1.   0.9]]\n",
            "weights at the beginning of this step\n",
            "weights[ 0 ]= [[ 0.1  0.3]\n",
            " [-0.4  0.2]]\n",
            "weights[ 1 ]= [-0.4  0.8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py:1768: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weights[ 2 ]= [[ 0.4  0.2]\n",
            " [-0.4  0.7]]\n",
            "weights[ 3 ]= [0.8 0.2]\n",
            "MSE with the initial weights: 1.442586204593552\n",
            "2) ===After applying GRADIENT:===\n",
            "------------- Results for step (iteration) = 0\n",
            "Gradients g to be used in new_weights = current_weights - learning_rate*g\n",
            "gradients[ 0 ]= [[-0.04047599  0.05397328]\n",
            " [-0.03372999  0.04497773]]\n",
            "gradients[ 1 ]= [ 0.03372999 -0.04497773]\n",
            "gradients[ 2 ]= [[ 0.17141584 -0.02460266]\n",
            " [ 0.20412011 -0.02929657]]\n",
            "gradients[ 3 ]= [ 0.36468667 -0.05234207]\n",
            "weights after gradient propagation in this step\n",
            "weights[ 0 ]= [[ 0.14047599  0.24602672]\n",
            " [-0.36627     0.15502226]]\n",
            "weights[ 1 ]= [-0.43373     0.84497774]\n",
            "weights[ 2 ]= [[ 0.22858417  0.22460265]\n",
            " [-0.60412014  0.72929657]]\n",
            "weights[ 3 ]= [0.43531334 0.25234208]\n",
            "MSE with the new weights: 1.214150265512243\n",
            "Learning rate =  1\n",
            " ############ Step = 1 ############\n",
            "1) ===BEFORE using any GRADIENT:===\n",
            "Network input [x1, x2]:\n",
            " [[-1.2 -1. ]]\n",
            "Network output [out_o1, out_o2]:\n",
            " [[0.5437048 0.6872184]]\n",
            "targets:\n",
            " [[-1.   0.9]]\n",
            "weights at the beginning of this step\n",
            "weights[ 0 ]= [[ 0.14047599  0.24602672]\n",
            " [-0.36627     0.15502226]]\n",
            "weights[ 1 ]= [-0.43373     0.84497774]\n",
            "weights[ 2 ]= [[ 0.22858417  0.22460265]\n",
            " [-0.60412014  0.72929657]]\n",
            "weights[ 3 ]= [0.43531334 0.25234208]\n",
            "MSE with the initial weights: 1.214150265512243\n",
            "2) ===After applying GRADIENT:===\n",
            "------------- Results for step (iteration) = 1\n",
            "Gradients g to be used in new_weights = current_weights - learning_rate*g\n",
            "gradients[ 0 ]= [[-0.02286109  0.07640134]\n",
            " [-0.01905091  0.06366778]]\n",
            "gradients[ 1 ]= [ 0.01905091 -0.06366778]\n",
            "gradients[ 2 ]= [[ 0.16899444 -0.02018222]\n",
            " [ 0.22879817 -0.02732431]]\n",
            "gradients[ 3 ]= [ 0.38297755 -0.04573724]\n",
            "weights after gradient propagation in this step\n",
            "weights[ 0 ]= [[ 0.16333708  0.16962539]\n",
            " [-0.3472191   0.09135448]]\n",
            "weights[ 1 ]= [-0.4527809  0.9086455]\n",
            "weights[ 2 ]= [[ 0.05958973  0.24478488]\n",
            " [-0.8329183   0.7566209 ]]\n",
            "weights[ 3 ]= [0.0523358 0.2980793]\n",
            "MSE with the new weights: 0.9791119337644435\n",
            "Learning rate =  1\n",
            " ############ Step = 2 ############\n",
            "1) ===BEFORE using any GRADIENT:===\n",
            "Network input [x1, x2]:\n",
            " [[-1.2 -1. ]]\n",
            "Network output [out_o1, out_o2]:\n",
            " [[0.38634074 0.70951843]]\n",
            "targets:\n",
            " [[-1.   0.9]]\n",
            "weights at the beginning of this step\n",
            "weights[ 0 ]= [[ 0.16333708  0.16962539]\n",
            " [-0.3472191   0.09135448]]\n",
            "weights[ 1 ]= [-0.4527809  0.9086455]\n",
            "weights[ 2 ]= [[ 0.05958973  0.24478488]\n",
            " [-0.8329183   0.7566209 ]]\n",
            "weights[ 3 ]= [0.0523358 0.2980793]\n",
            "MSE with the initial weights: 0.9791119337644435\n",
            "2) ===After applying GRADIENT:===\n",
            "------------- Results for step (iteration) = 2\n",
            "Gradients g to be used in new_weights = current_weights - learning_rate*g\n",
            "gradients[ 0 ]= [[-0.00292571  0.08297694]\n",
            " [-0.00243809  0.06914745]]\n",
            "gradients[ 1 ]= [ 0.00243809 -0.06914745]\n",
            "gradients[ 2 ]= [[ 0.13974464 -0.01669178]\n",
            " [ 0.21324277 -0.02547075]]\n",
            "gradients[ 3 ]= [ 0.32867584 -0.03925863]\n",
            "weights after gradient propagation in this step\n",
            "weights[ 0 ]= [[ 0.16626279  0.08664844]\n",
            " [-0.344781    0.02220703]]\n",
            "weights[ 1 ]= [-0.455219  0.977793]\n",
            "weights[ 2 ]= [[-0.08015491  0.26147667]\n",
            " [-1.046161    0.7820916 ]]\n",
            "weights[ 3 ]= [-0.27634004  0.33733794]\n",
            "MSE with the new weights: 0.8087944248404987\n",
            "\n",
            " ===AFTER executing all GRADIENT descent steps===\n",
            "final weights[ 0 ]= [[ 0.16626279  0.08664844]\n",
            " [-0.344781    0.02220703]]\n",
            "final weights[ 1 ]= [-0.455219  0.977793]\n",
            "final weights[ 2 ]= [[-0.08015491  0.26147667]\n",
            " [-1.046161    0.7820916 ]]\n",
            "final weights[ 3 ]= [-0.27634004  0.33733794]\n",
            "outputs:\n",
            " [[0.26047248 0.7303003 ]]\n",
            "targets:\n",
            " [[-1.   0.9]]\n",
            "Final MSE =  0.8087944248404987\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras import backend as k\n",
        "from keras import losses\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#Initialize all weights to match the values adopted in the example at\n",
        "#https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/ \n",
        "#Initialise kernel (weights matrix) to required value\n",
        "def kernel_init_layer1(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (2, 2) \n",
        "    # and create a fixed array with this dimension for layer 1\n",
        "    kernel = np.array([[0.1, 0.3],[-0.4, 0.2]])\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (weights matrix) to required value\n",
        "def kernel_init_layer2(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (2, 2)\n",
        "    # and create a fixed array with this dimension for layer 2\n",
        "    kernel = np.array([[0.4, 0.2], [-0.4, 0.7]])\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (bias vector) to required value\n",
        "def kernel_init_bias1(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (1, 2)\n",
        "    # and create a fixed array with this dimension for the \n",
        "    # bias vector of layer 1\n",
        "    kernel = np.array([-0.4, 0.8]) #both neurons with the same value\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (bias vector) to required value\n",
        "def kernel_init_bias2(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (1, 2)\n",
        "    # and create a fixed array with this dimension for the \n",
        "    # bias vector of layer 2\n",
        "    kernel = np.array([0.8, 0.2]) #both neurons with the same value\n",
        "    return kernel \n",
        "\n",
        "# from https://stackoverflow.com/questions/66221788/tf-gradients-is-not-supported-when-eager-execution-is-enabled-use-tf-gradientta\n",
        "# TF 2 does not use \"eager\" execution, so disable it:\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "#Define the neural network model with dense layers. Syntax:\n",
        "#https://keras.io/api/layers/core_layers/dense/\n",
        "#The sigmoid activation function in Keras is the standard logistic function 1/(1+exp(-x)).\n",
        "#https://keras.io/api/layers/activations/\n",
        "model = Sequential()\n",
        "model.add(Dense(2, input_dim=2, use_bias=True,  bias_initializer=kernel_init_bias1,\n",
        "        kernel_initializer=kernel_init_layer1, activation='sigmoid'))\n",
        "model.add(Dense(2, use_bias=True,  bias_initializer=kernel_init_bias2, \n",
        "        kernel_initializer=kernel_init_layer2, activation='sigmoid'))\n",
        "model.summary() # display the architecture\n",
        "\n",
        "# We are not going to use a Keras optimizer. Define a learning rate:\n",
        "learning_rate = 1  #you can change to 0.5 or any other reasonable value\n",
        "\n",
        "#now compile the model, informing loss and performanc metrics that\n",
        "#should be computed along the training\n",
        "model.compile(loss='mse', metrics=['accuracy'])\n",
        "\n",
        "# Begin TensorFlow\n",
        "sess = tf.compat.v1.InteractiveSession()\n",
        "sess.run(tf.compat.v1.initialize_all_variables())\n",
        "\n",
        "model.optimizer.lr = learning_rate\n",
        "\n",
        "print(\"===START TRAINING THE NETWORK:===\")\n",
        "steps = 3 # steps of gradient descent\n",
        "for s in range(steps):\n",
        "    print('Learning rate = ', k.get_value(model.optimizer.lr))\n",
        "    print(\" ############ Step = \" + str(s) + \" ############\")\n",
        "    print(\"1) ===BEFORE using any GRADIENT:===\")\n",
        "    #define input and target vectors, and also inform it to the loss function object\n",
        "    #in this code we will keep the same inputs and targets along the steps, but the inputs and\n",
        "    #targets can change when we go, for instance, through the training set\n",
        "    inputs = np.array([[-1.2,-1]]) #define the input for the current iteration (step)\n",
        "    outputs = model.predict(inputs) #forward pass\n",
        "    #define the target vector for the current iteration (step)\n",
        "    targets = np.array([[-1, 0.9]]) #notice that a sigmoid can output within range [0, 1]\n",
        "    mse = mean_squared_error(targets, outputs) #calculate MSE\n",
        "    #initialize loss object to be later incorporated to the gradients object that\n",
        "    #enables the calculation of the symbolic gradients\n",
        "    loss = losses.mean_squared_error(targets, model.output)\n",
        "    #  ===== Obtain symbolic gradient to calculate numerical gradients =====\n",
        "    gradients = k.gradients(loss, model.trainable_weights) #inform loss and weights\n",
        "    if False: #enable with True in case you want to see the objects\n",
        "        print(\"List of tensors representing the symbolic gradients:\")\n",
        "        for i in range(len(gradients)):\n",
        "            print('symbolic gradient[',i,']=',gradients[i])\n",
        "    print('Network input [x1, x2]:\\n', inputs)\n",
        "    print('Network output [out_o1, out_o2]:\\n', outputs)\n",
        "    print(\"targets:\\n\", targets)\n",
        "    #show weights at the beginning of iteration s\n",
        "    print('weights at the beginning of this step')\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        #note that model.trainable_weights[i] is an object of\n",
        "        # <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable\n",
        "        #therefore (see e.g. https://stackoverflow.com/questions/33679382/how-do-i-get-the-current-value-of-a-variable )\n",
        "        #you need to obtain its value via a TF session:\n",
        "        print('weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "    print(\"MSE with the initial weights:\", mse)\n",
        "\n",
        "    print(\"2) ===After applying GRADIENT:===\")\n",
        "    print(\"------------- Results for step (iteration) =\", s)\n",
        "    # ===== Calculate numerical gradient from symbolic gradients =====\n",
        "    # evaluated_gradients is a list, show its contents:\n",
        "    evaluated_gradients = sess.run(gradients, feed_dict={model.input: inputs})\n",
        "    print('Gradients g to be used in new_weights = current_weights - learning_rate*g')\n",
        "    for i in range(len(evaluated_gradients)):\n",
        "        print('gradients[',i,']=',evaluated_gradients[i])\n",
        "\n",
        "    # Apply (\"step down\") the gradient for each layer, subtracting the gradients\n",
        "    # from current weights scaled by the learning rate:\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        sess.run(tf.compat.v1.assign_sub(model.trainable_weights[i], learning_rate*evaluated_gradients[i]))\n",
        "\n",
        "    #show weights after gradient propagation of iteration s\n",
        "    print('weights after gradient propagation in this step')\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        print('weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "\n",
        "    # print the MSE with new weights:\n",
        "    outputs = model.predict(inputs)\n",
        "    mse = mean_squared_error(targets, outputs)\n",
        "    print(\"MSE with the new weights:\", mse)\n",
        "\n",
        "#Collect and show final results\n",
        "final_outputs = model.predict(inputs)\n",
        "final_mse = mean_squared_error(targets, final_outputs)\n",
        "\n",
        "print(\"\\n ===AFTER executing all GRADIENT descent steps===\")\n",
        "#show weights at the end of training\n",
        "for i in range(len(model.trainable_weights)):\n",
        "    #note that model.trainable_weights[i] is an object of\n",
        "    # <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable\n",
        "    #therefore (see e.g. https://stackoverflow.com/questions/33679382/how-do-i-get-the-current-value-of-a-variable )\n",
        "    #you need to obtain its value via a TF session:\n",
        "    print('final weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "print(\"outputs:\\n\", final_outputs)\n",
        "print(\"targets:\\n\", targets)\n",
        "print(\"Final MSE = \", final_mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Para Learning rate = 0.5 amostra_2 -1.2,-1,-1,0.9"
      ],
      "metadata": {
        "id": "Ig5OyzMjsqAt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e6d7416-80f3-46d0-dc81-297399ec35ed",
        "id": "yudBYdCDstfs"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_12 (Dense)            (None, 2)                 6         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 2)                 6         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12\n",
            "Trainable params: 12\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "===START TRAINING THE NETWORK:===\n",
            "Learning rate =  0.5\n",
            " ############ Step = 0 ############\n",
            "1) ===BEFORE using any GRADIENT:===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py:1768: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network input [x1, x2]:\n",
            " [[-1.2 -1. ]]\n",
            "Network output [out_o1, out_o2]:\n",
            " [[0.6822494  0.66503346]]\n",
            "targets:\n",
            " [[-1.   0.9]]\n",
            "weights at the beginning of this step\n",
            "weights[ 0 ]= [[ 0.1  0.3]\n",
            " [-0.4  0.2]]\n",
            "weights[ 1 ]= [-0.4  0.8]\n",
            "weights[ 2 ]= [[ 0.4  0.2]\n",
            " [-0.4  0.7]]\n",
            "weights[ 3 ]= [0.8 0.2]\n",
            "MSE with the initial weights: 1.442586204593552\n",
            "2) ===After applying GRADIENT:===\n",
            "------------- Results for step (iteration) = 0\n",
            "Gradients g to be used in new_weights = current_weights - learning_rate*g\n",
            "gradients[ 0 ]= [[-0.04047599  0.05397328]\n",
            " [-0.03372999  0.04497773]]\n",
            "gradients[ 1 ]= [ 0.03372999 -0.04497773]\n",
            "gradients[ 2 ]= [[ 0.17141584 -0.02460266]\n",
            " [ 0.20412011 -0.02929657]]\n",
            "gradients[ 3 ]= [ 0.36468667 -0.05234207]\n",
            "weights after gradient propagation in this step\n",
            "weights[ 0 ]= [[ 0.120238    0.27301338]\n",
            " [-0.38313502  0.17751114]]\n",
            "weights[ 1 ]= [-0.416865   0.8224889]\n",
            "weights[ 2 ]= [[ 0.31429207  0.21230133]\n",
            " [-0.50206006  0.71464825]]\n",
            "weights[ 3 ]= [0.6176567  0.22617105]\n",
            "MSE with the new weights: 1.3298933726006517\n",
            "Learning rate =  0.5\n",
            " ############ Step = 1 ############\n",
            "1) ===BEFORE using any GRADIENT:===\n",
            "Network input [x1, x2]:\n",
            " [[-1.2 -1. ]]\n",
            "Network output [out_o1, out_o2]:\n",
            " [[0.6154594  0.67621946]]\n",
            "targets:\n",
            " [[-1.   0.9]]\n",
            "weights at the beginning of this step\n",
            "weights[ 0 ]= [[ 0.120238    0.27301338]\n",
            " [-0.38313502  0.17751114]]\n",
            "weights[ 1 ]= [-0.416865   0.8224889]\n",
            "weights[ 2 ]= [[ 0.31429207  0.21230133]\n",
            " [-0.50206006  0.71464825]]\n",
            "weights[ 3 ]= [0.6176567  0.22617105]\n",
            "MSE with the initial weights: 1.3298933726006517\n",
            "2) ===After applying GRADIENT:===\n",
            "------------- Results for step (iteration) = 1\n",
            "Gradients g to be used in new_weights = current_weights - learning_rate*g\n",
            "gradients[ 0 ]= [[-0.03266886  0.06640406]\n",
            " [-0.02722404  0.05533671]]\n",
            "gradients[ 1 ]= [ 0.02722404 -0.05533671]\n",
            "gradients[ 2 ]= [[ 0.17419434 -0.02232323]\n",
            " [ 0.2212468  -0.02835306]]\n",
            "gradients[ 3 ]= [ 0.38232937 -0.04899601]\n",
            "weights after gradient propagation in this step\n",
            "weights[ 0 ]= [[ 0.13657242  0.23981136]\n",
            " [-0.369523    0.14984278]]\n",
            "weights[ 1 ]= [-0.43047702  0.85015726]\n",
            "weights[ 2 ]= [[ 0.2271949   0.22346294]\n",
            " [-0.6126835   0.7288248 ]]\n",
            "weights[ 3 ]= [0.42649204 0.25066906]\n",
            "MSE with the new weights: 1.2077598933315175\n",
            "Learning rate =  0.5\n",
            " ############ Step = 2 ############\n",
            "1) ===BEFORE using any GRADIENT:===\n",
            "Network input [x1, x2]:\n",
            " [[-1.2 -1. ]]\n",
            "Network output [out_o1, out_o2]:\n",
            " [[0.5395975  0.68749285]]\n",
            "targets:\n",
            " [[-1.   0.9]]\n",
            "weights at the beginning of this step\n",
            "weights[ 0 ]= [[ 0.13657242  0.23981136]\n",
            " [-0.369523    0.14984278]]\n",
            "weights[ 1 ]= [-0.43047702  0.85015726]\n",
            "weights[ 2 ]= [[ 0.2271949   0.22346294]\n",
            " [-0.6126835   0.7288248 ]]\n",
            "weights[ 3 ]= [0.42649204 0.25066906]\n",
            "MSE with the initial weights: 1.2077598933315175\n",
            "2) ===After applying GRADIENT:===\n",
            "------------- Results for step (iteration) = 2\n",
            "Gradients g to be used in new_weights = current_weights - learning_rate*g\n",
            "gradients[ 0 ]= [[-0.0227205   0.07696405]\n",
            " [-0.01893375  0.06413671]]\n",
            "gradients[ 1 ]= [ 0.01893375 -0.06413671]\n",
            "gradients[ 2 ]= [[ 0.16983318 -0.0202726 ]\n",
            " [ 0.23014027 -0.02747132]]\n",
            "gradients[ 3 ]= [ 0.38248533 -0.0456564 ]\n",
            "weights after gradient propagation in this step\n",
            "weights[ 0 ]= [[ 0.14793266  0.20132934]\n",
            " [-0.3600561   0.11777443]]\n",
            "weights[ 1 ]= [-0.4399439   0.88222563]\n",
            "weights[ 2 ]= [[ 0.14227831  0.23359925]\n",
            " [-0.72775364  0.74256045]]\n",
            "weights[ 3 ]= [0.23524937 0.27349725]\n",
            "MSE with the new weights: 1.086307958175166\n",
            "\n",
            " ===AFTER executing all GRADIENT descent steps===\n",
            "final weights[ 0 ]= [[ 0.14793266  0.20132934]\n",
            " [-0.3600561   0.11777443]]\n",
            "final weights[ 1 ]= [-0.4399439   0.88222563]\n",
            "final weights[ 2 ]= [[ 0.14227831  0.23359925]\n",
            " [-0.72775364  0.74256045]]\n",
            "final weights[ 3 ]= [0.23524937 0.27349725]\n",
            "outputs:\n",
            " [[0.4601815  0.69878894]]\n",
            "targets:\n",
            " [[-1.   0.9]]\n",
            "Final MSE =  1.086307958175166\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras import backend as k\n",
        "from keras import losses\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#Initialize all weights to match the values adopted in the example at\n",
        "#https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/ \n",
        "#Initialise kernel (weights matrix) to required value\n",
        "def kernel_init_layer1(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (2, 2) \n",
        "    # and create a fixed array with this dimension for layer 1\n",
        "    kernel = np.array([[0.1, 0.3],[-0.4, 0.2]])\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (weights matrix) to required value\n",
        "def kernel_init_layer2(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (2, 2)\n",
        "    # and create a fixed array with this dimension for layer 2\n",
        "    kernel = np.array([[0.4, 0.2], [-0.4, 0.7]])\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (bias vector) to required value\n",
        "def kernel_init_bias1(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (1, 2)\n",
        "    # and create a fixed array with this dimension for the \n",
        "    # bias vector of layer 1\n",
        "    kernel = np.array([-0.4, 0.8]) #both neurons with the same value\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (bias vector) to required value\n",
        "def kernel_init_bias2(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (1, 2)\n",
        "    # and create a fixed array with this dimension for the \n",
        "    # bias vector of layer 2\n",
        "    kernel = np.array([0.8, 0.2]) #both neurons with the same value\n",
        "    return kernel \n",
        "\n",
        "# from https://stackoverflow.com/questions/66221788/tf-gradients-is-not-supported-when-eager-execution-is-enabled-use-tf-gradientta\n",
        "# TF 2 does not use \"eager\" execution, so disable it:\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "#Define the neural network model with dense layers. Syntax:\n",
        "#https://keras.io/api/layers/core_layers/dense/\n",
        "#The sigmoid activation function in Keras is the standard logistic function 1/(1+exp(-x)).\n",
        "#https://keras.io/api/layers/activations/\n",
        "model = Sequential()\n",
        "model.add(Dense(2, input_dim=2, use_bias=True,  bias_initializer=kernel_init_bias1,\n",
        "        kernel_initializer=kernel_init_layer1, activation='sigmoid'))\n",
        "model.add(Dense(2, use_bias=True,  bias_initializer=kernel_init_bias2, \n",
        "        kernel_initializer=kernel_init_layer2, activation='sigmoid'))\n",
        "model.summary() # display the architecture\n",
        "\n",
        "# We are not going to use a Keras optimizer. Define a learning rate:\n",
        "learning_rate = 0.5  #you can change to 0.5 or any other reasonable value\n",
        "\n",
        "#now compile the model, informing loss and performanc metrics that\n",
        "#should be computed along the training\n",
        "model.compile(loss='mse', metrics=['accuracy'])\n",
        "\n",
        "# Begin TensorFlow\n",
        "sess = tf.compat.v1.InteractiveSession()\n",
        "sess.run(tf.compat.v1.initialize_all_variables())\n",
        "\n",
        "model.optimizer.lr = learning_rate\n",
        "\n",
        "print(\"===START TRAINING THE NETWORK:===\")\n",
        "steps = 3 # steps of gradient descent\n",
        "for s in range(steps):\n",
        "    print('Learning rate = ', k.get_value(model.optimizer.lr))\n",
        "    print(\" ############ Step = \" + str(s) + \" ############\")\n",
        "    print(\"1) ===BEFORE using any GRADIENT:===\")\n",
        "    #define input and target vectors, and also inform it to the loss function object\n",
        "    #in this code we will keep the same inputs and targets along the steps, but the inputs and\n",
        "    #targets can change when we go, for instance, through the training set\n",
        "    inputs = np.array([[-1.2,-1]]) #define the input for the current iteration (step)\n",
        "    outputs = model.predict(inputs) #forward pass\n",
        "    #define the target vector for the current iteration (step)\n",
        "    targets = np.array([[-1, 0.9]]) #notice that a sigmoid can output within range [0, 1]\n",
        "    mse = mean_squared_error(targets, outputs) #calculate MSE\n",
        "    #initialize loss object to be later incorporated to the gradients object that\n",
        "    #enables the calculation of the symbolic gradients\n",
        "    loss = losses.mean_squared_error(targets, model.output)\n",
        "    #  ===== Obtain symbolic gradient to calculate numerical gradients =====\n",
        "    gradients = k.gradients(loss, model.trainable_weights) #inform loss and weights\n",
        "    if False: #enable with True in case you want to see the objects\n",
        "        print(\"List of tensors representing the symbolic gradients:\")\n",
        "        for i in range(len(gradients)):\n",
        "            print('symbolic gradient[',i,']=',gradients[i])\n",
        "    print('Network input [x1, x2]:\\n', inputs)\n",
        "    print('Network output [out_o1, out_o2]:\\n', outputs)\n",
        "    print(\"targets:\\n\", targets)\n",
        "    #show weights at the beginning of iteration s\n",
        "    print('weights at the beginning of this step')\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        #note that model.trainable_weights[i] is an object of\n",
        "        # <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable\n",
        "        #therefore (see e.g. https://stackoverflow.com/questions/33679382/how-do-i-get-the-current-value-of-a-variable )\n",
        "        #you need to obtain its value via a TF session:\n",
        "        print('weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "    print(\"MSE with the initial weights:\", mse)\n",
        "\n",
        "    print(\"2) ===After applying GRADIENT:===\")\n",
        "    print(\"------------- Results for step (iteration) =\", s)\n",
        "    # ===== Calculate numerical gradient from symbolic gradients =====\n",
        "    # evaluated_gradients is a list, show its contents:\n",
        "    evaluated_gradients = sess.run(gradients, feed_dict={model.input: inputs})\n",
        "    print('Gradients g to be used in new_weights = current_weights - learning_rate*g')\n",
        "    for i in range(len(evaluated_gradients)):\n",
        "        print('gradients[',i,']=',evaluated_gradients[i])\n",
        "\n",
        "    # Apply (\"step down\") the gradient for each layer, subtracting the gradients\n",
        "    # from current weights scaled by the learning rate:\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        sess.run(tf.compat.v1.assign_sub(model.trainable_weights[i], learning_rate*evaluated_gradients[i]))\n",
        "\n",
        "    #show weights after gradient propagation of iteration s\n",
        "    print('weights after gradient propagation in this step')\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        print('weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "\n",
        "    # print the MSE with new weights:\n",
        "    outputs = model.predict(inputs)\n",
        "    mse = mean_squared_error(targets, outputs)\n",
        "    print(\"MSE with the new weights:\", mse)\n",
        "\n",
        "#Collect and show final results\n",
        "final_outputs = model.predict(inputs)\n",
        "final_mse = mean_squared_error(targets, final_outputs)\n",
        "\n",
        "print(\"\\n ===AFTER executing all GRADIENT descent steps===\")\n",
        "#show weights at the end of training\n",
        "for i in range(len(model.trainable_weights)):\n",
        "    #note that model.trainable_weights[i] is an object of\n",
        "    # <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable\n",
        "    #therefore (see e.g. https://stackoverflow.com/questions/33679382/how-do-i-get-the-current-value-of-a-variable )\n",
        "    #you need to obtain its value via a TF session:\n",
        "    print('final weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "print(\"outputs:\\n\", final_outputs)\n",
        "print(\"targets:\\n\", targets)\n",
        "print(\"Final MSE = \", final_mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Codigo de aplicaÃ§Ã£o para a amostra_3 (1.2,0,0,0.9)"
      ],
      "metadata": {
        "id": "Py6H4jlkGKox"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Para Learning rate = 1"
      ],
      "metadata": {
        "id": "IytorQtOCdyP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b7b6445-e078-46f6-b4b0-fd32798705b2",
        "id": "eZ1v9ynfxgme"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_6 (Dense)             (None, 2)                 6         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 2)                 6         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12\n",
            "Trainable params: 12\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "===START TRAINING THE NETWORK:===\n",
            "Learning rate =  1\n",
            " ############ Step = 0 ############\n",
            "1) ===BEFORE using any GRADIENT:===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py:1768: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network input [x1, x2]:\n",
            " [[1.2 0. ]]\n",
            "Network output [out_o1, out_o2]:\n",
            " [[0.6609764 0.6940291]]\n",
            "targets:\n",
            " [[0.  0.9]]\n",
            "weights at the beginning of this step\n",
            "weights[ 0 ]= [[ 0.1  0.3]\n",
            " [-0.4  0.2]]\n",
            "weights[ 1 ]= [-0.4  0.8]\n",
            "weights[ 2 ]= [[ 0.4  0.2]\n",
            " [-0.4  0.7]]\n",
            "weights[ 3 ]= [0.8 0.2]\n",
            "MSE with the initial weights: 0.23965691453887417\n",
            "2) ===After applying GRADIENT:===\n",
            "------------- Results for step (iteration) = 0\n",
            "Gradients g to be used in new_weights = current_weights - learning_rate*g\n",
            "gradients[ 0 ]= [[ 0.01485651 -0.01959436]\n",
            " [ 0.         -0.        ]]\n",
            "gradients[ 1 ]= [ 0.01238043 -0.01632863]\n",
            "gradients[ 2 ]= [[ 0.06375708 -0.01882739]\n",
            " [ 0.11276554 -0.03329954]]\n",
            "gradients[ 3 ]= [ 0.14811596 -0.04373848]\n",
            "weights after gradient propagation in this step\n",
            "weights[ 0 ]= [[ 0.08514349  0.31959438]\n",
            " [-0.4         0.2       ]]\n",
            "weights[ 1 ]= [-0.41238043  0.81632864]\n",
            "weights[ 2 ]= [[ 0.3362429  0.2188274]\n",
            " [-0.5127655  0.7332995]]\n",
            "weights[ 3 ]= [0.6518841  0.24373847]\n",
            "MSE with the new weights: 0.1971003508391678\n",
            "Learning rate =  1\n",
            " ############ Step = 1 ############\n",
            "1) ===BEFORE using any GRADIENT:===\n",
            "Network input [x1, x2]:\n",
            " [[1.2 0. ]]\n",
            "Network output [out_o1, out_o2]:\n",
            " [[0.59870636 0.7109196 ]]\n",
            "targets:\n",
            " [[0.  0.9]]\n",
            "weights at the beginning of this step\n",
            "weights[ 0 ]= [[ 0.08514349  0.31959438]\n",
            " [-0.4         0.2       ]]\n",
            "weights[ 1 ]= [-0.41238043  0.81632864]\n",
            "weights[ 2 ]= [[ 0.3362429  0.2188274]\n",
            " [-0.5127655  0.7332995]]\n",
            "weights[ 3 ]= [0.6518841  0.24373847]\n",
            "MSE with the initial weights: 0.1971003508391678\n",
            "2) ===After applying GRADIENT:===\n",
            "------------- Results for step (iteration) = 1\n",
            "Gradients g to be used in new_weights = current_weights - learning_rate*g\n",
            "gradients[ 0 ]= [[ 0.01167577 -0.02183011]\n",
            " [ 0.         -0.        ]]\n",
            "gradients[ 1 ]= [ 0.0097298  -0.01819176]\n",
            "gradients[ 2 ]= [[ 0.06085497 -0.01643961]\n",
            " [ 0.11054319 -0.02986259]]\n",
            "gradients[ 3 ]= [ 0.14384343 -0.03885845]\n",
            "weights after gradient propagation in this step\n",
            "weights[ 0 ]= [[ 0.07346772  0.3414245 ]\n",
            " [-0.4         0.2       ]]\n",
            "weights[ 1 ]= [-0.42211023  0.8345204 ]\n",
            "weights[ 2 ]= [[ 0.27538794  0.23526701]\n",
            " [-0.6233087   0.7631621 ]]\n",
            "weights[ 3 ]= [0.50804067 0.28259692]\n",
            "MSE with the new weights: 0.15813955485736736\n",
            "Learning rate =  1\n",
            " ############ Step = 2 ############\n",
            "1) ===BEFORE using any GRADIENT:===\n",
            "Network input [x1, x2]:\n",
            " [[1.2 0. ]]\n",
            "Network output [out_o1, out_o2]:\n",
            " [[0.53471416 0.72575915]]\n",
            "targets:\n",
            " [[0.  0.9]]\n",
            "weights at the beginning of this step\n",
            "weights[ 0 ]= [[ 0.07346772  0.3414245 ]\n",
            " [-0.4         0.2       ]]\n",
            "weights[ 1 ]= [-0.42211023  0.8345204 ]\n",
            "weights[ 2 ]= [[ 0.27538794  0.23526701]\n",
            " [-0.6233087   0.7631621 ]]\n",
            "weights[ 3 ]= [0.50804067 0.28259692]\n",
            "MSE with the initial weights: 0.15813955485736736\n",
            "2) ===After applying GRADIENT:===\n",
            "------------- Results for step (iteration) = 2\n",
            "Gradients g to be used in new_weights = current_weights - learning_rate*g\n",
            "gradients[ 0 ]= [[ 0.00830928 -0.02279531]\n",
            " [ 0.         -0.        ]]\n",
            "gradients[ 1 ]= [ 0.0069244  -0.01899609]\n",
            "gradients[ 2 ]= [[ 0.05551251 -0.01447112]\n",
            " [ 0.10327437 -0.02692179]]\n",
            "gradients[ 3 ]= [ 0.13303418 -0.03467964]\n",
            "weights after gradient propagation in this step\n",
            "weights[ 0 ]= [[ 0.06515844  0.3642198 ]\n",
            " [-0.4         0.2       ]]\n",
            "weights[ 1 ]= [-0.42903462  0.85351646]\n",
            "weights[ 2 ]= [[ 0.21987543  0.24973814]\n",
            " [-0.72658306  0.7900839 ]]\n",
            "weights[ 3 ]= [0.3750065  0.31727657]\n",
            "MSE with the new weights: 0.1253353008554971\n",
            "\n",
            " ===AFTER executing all GRADIENT descent steps===\n",
            "final weights[ 0 ]= [[ 0.06515844  0.3642198 ]\n",
            " [-0.4         0.2       ]]\n",
            "final weights[ 1 ]= [-0.42903462  0.85351646]\n",
            "final weights[ 2 ]= [[ 0.21987543  0.24973814]\n",
            " [-0.72658306  0.7900839 ]]\n",
            "final weights[ 3 ]= [0.3750065  0.31727657]\n",
            "outputs:\n",
            " [[0.47403207 0.7388659 ]]\n",
            "targets:\n",
            " [[0.  0.9]]\n",
            "Final MSE =  0.1253353008554971\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras import backend as k\n",
        "from keras import losses\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#Initialize all weights to match the values adopted in the example at\n",
        "#https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/ \n",
        "#Initialise kernel (weights matrix) to required value\n",
        "def kernel_init_layer1(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (2, 2) \n",
        "    # and create a fixed array with this dimension for layer 1\n",
        "    kernel = np.array([[0.1, 0.3],[-0.4, 0.2]])\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (weights matrix) to required value\n",
        "def kernel_init_layer2(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (2, 2)\n",
        "    # and create a fixed array with this dimension for layer 2\n",
        "    kernel = np.array([[0.4, 0.2], [-0.4, 0.7]])\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (bias vector) to required value\n",
        "def kernel_init_bias1(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (1, 2)\n",
        "    # and create a fixed array with this dimension for the \n",
        "    # bias vector of layer 1\n",
        "    kernel = np.array([-0.4, 0.8]) #both neurons with the same value\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (bias vector) to required value\n",
        "def kernel_init_bias2(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (1, 2)\n",
        "    # and create a fixed array with this dimension for the \n",
        "    # bias vector of layer 2\n",
        "    kernel = np.array([0.8, 0.2]) #both neurons with the same value\n",
        "    return kernel \n",
        "\n",
        "# from https://stackoverflow.com/questions/66221788/tf-gradients-is-not-supported-when-eager-execution-is-enabled-use-tf-gradientta\n",
        "# TF 2 does not use \"eager\" execution, so disable it:\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "#Define the neural network model with dense layers. Syntax:\n",
        "#https://keras.io/api/layers/core_layers/dense/\n",
        "#The sigmoid activation function in Keras is the standard logistic function 1/(1+exp(-x)).\n",
        "#https://keras.io/api/layers/activations/\n",
        "model = Sequential()\n",
        "model.add(Dense(2, input_dim=2, use_bias=True,  bias_initializer=kernel_init_bias1,\n",
        "        kernel_initializer=kernel_init_layer1, activation='sigmoid'))\n",
        "model.add(Dense(2, use_bias=True,  bias_initializer=kernel_init_bias2, \n",
        "        kernel_initializer=kernel_init_layer2, activation='sigmoid'))\n",
        "model.summary() # display the architecture\n",
        "\n",
        "# We are not going to use a Keras optimizer. Define a learning rate:\n",
        "learning_rate = 1  #you can change to 0.5 or any other reasonable value\n",
        "\n",
        "#now compile the model, informing loss and performanc metrics that\n",
        "#should be computed along the training\n",
        "model.compile(loss='mse', metrics=['accuracy'])\n",
        "\n",
        "# Begin TensorFlow\n",
        "sess = tf.compat.v1.InteractiveSession()\n",
        "sess.run(tf.compat.v1.initialize_all_variables())\n",
        "\n",
        "model.optimizer.lr = learning_rate\n",
        "\n",
        "print(\"===START TRAINING THE NETWORK:===\")\n",
        "steps = 3 # steps of gradient descent\n",
        "for s in range(steps):\n",
        "    print('Learning rate = ', k.get_value(model.optimizer.lr))\n",
        "    print(\" ############ Step = \" + str(s) + \" ############\")\n",
        "    print(\"1) ===BEFORE using any GRADIENT:===\")\n",
        "    #define input and target vectors, and also inform it to the loss function object\n",
        "    #in this code we will keep the same inputs and targets along the steps, but the inputs and\n",
        "    #targets can change when we go, for instance, through the training set\n",
        "    inputs = np.array([[1.2,0]]) #define the input for the current iteration (step)\n",
        "    outputs = model.predict(inputs) #forward pass\n",
        "    #define the target vector for the current iteration (step)\n",
        "    targets = np.array([[0, 0.9]]) #notice that a sigmoid can output within range [0, 1]\n",
        "    mse = mean_squared_error(targets, outputs) #calculate MSE\n",
        "    #initialize loss object to be later incorporated to the gradients object that\n",
        "    #enables the calculation of the symbolic gradients\n",
        "    loss = losses.mean_squared_error(targets, model.output)\n",
        "    #  ===== Obtain symbolic gradient to calculate numerical gradients =====\n",
        "    gradients = k.gradients(loss, model.trainable_weights) #inform loss and weights\n",
        "    if False: #enable with True in case you want to see the objects\n",
        "        print(\"List of tensors representing the symbolic gradients:\")\n",
        "        for i in range(len(gradients)):\n",
        "            print('symbolic gradient[',i,']=',gradients[i])\n",
        "    print('Network input [x1, x2]:\\n', inputs)\n",
        "    print('Network output [out_o1, out_o2]:\\n', outputs)\n",
        "    print(\"targets:\\n\", targets)\n",
        "    #show weights at the beginning of iteration s\n",
        "    print('weights at the beginning of this step')\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        #note that model.trainable_weights[i] is an object of\n",
        "        # <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable\n",
        "        #therefore (see e.g. https://stackoverflow.com/questions/33679382/how-do-i-get-the-current-value-of-a-variable )\n",
        "        #you need to obtain its value via a TF session:\n",
        "        print('weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "    print(\"MSE with the initial weights:\", mse)\n",
        "\n",
        "    print(\"2) ===After applying GRADIENT:===\")\n",
        "    print(\"------------- Results for step (iteration) =\", s)\n",
        "    # ===== Calculate numerical gradient from symbolic gradients =====\n",
        "    # evaluated_gradients is a list, show its contents:\n",
        "    evaluated_gradients = sess.run(gradients, feed_dict={model.input: inputs})\n",
        "    print('Gradients g to be used in new_weights = current_weights - learning_rate*g')\n",
        "    for i in range(len(evaluated_gradients)):\n",
        "        print('gradients[',i,']=',evaluated_gradients[i])\n",
        "\n",
        "    # Apply (\"step down\") the gradient for each layer, subtracting the gradients\n",
        "    # from current weights scaled by the learning rate:\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        sess.run(tf.compat.v1.assign_sub(model.trainable_weights[i], learning_rate*evaluated_gradients[i]))\n",
        "\n",
        "    #show weights after gradient propagation of iteration s\n",
        "    print('weights after gradient propagation in this step')\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        print('weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "\n",
        "    # print the MSE with new weights:\n",
        "    outputs = model.predict(inputs)\n",
        "    mse = mean_squared_error(targets, outputs)\n",
        "    print(\"MSE with the new weights:\", mse)\n",
        "\n",
        "#Collect and show final results\n",
        "final_outputs = model.predict(inputs)\n",
        "final_mse = mean_squared_error(targets, final_outputs)\n",
        "\n",
        "print(\"\\n ===AFTER executing all GRADIENT descent steps===\")\n",
        "#show weights at the end of training\n",
        "for i in range(len(model.trainable_weights)):\n",
        "    #note that model.trainable_weights[i] is an object of\n",
        "    # <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable\n",
        "    #therefore (see e.g. https://stackoverflow.com/questions/33679382/how-do-i-get-the-current-value-of-a-variable )\n",
        "    #you need to obtain its value via a TF session:\n",
        "    print('final weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "print(\"outputs:\\n\", final_outputs)\n",
        "print(\"targets:\\n\", targets)\n",
        "print(\"Final MSE = \", final_mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Para Learning rate = 1=0.5"
      ],
      "metadata": {
        "id": "_-y3eiLN2v-Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d590d96-d783-4f5e-dfdd-db79d5d29412",
        "id": "7c5-E0w03CRI"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_14 (Dense)            (None, 2)                 6         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 2)                 6         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12\n",
            "Trainable params: 12\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "===START TRAINING THE NETWORK:===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py:1768: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning rate =  0.5\n",
            " ############ Step = 0 ############\n",
            "1) ===BEFORE using any GRADIENT:===\n",
            "Network input [x1, x2]:\n",
            " [[1.2 0. ]]\n",
            "Network output [out_o1, out_o2]:\n",
            " [[0.6609764 0.6940291]]\n",
            "targets:\n",
            " [[0.  0.9]]\n",
            "weights at the beginning of this step\n",
            "weights[ 0 ]= [[ 0.1  0.3]\n",
            " [-0.4  0.2]]\n",
            "weights[ 1 ]= [-0.4  0.8]\n",
            "weights[ 2 ]= [[ 0.4  0.2]\n",
            " [-0.4  0.7]]\n",
            "weights[ 3 ]= [0.8 0.2]\n",
            "MSE with the initial weights: 0.23965691453887417\n",
            "2) ===After applying GRADIENT:===\n",
            "------------- Results for step (iteration) = 0\n",
            "Gradients g to be used in new_weights = current_weights - learning_rate*g\n",
            "gradients[ 0 ]= [[ 0.01485651 -0.01959436]\n",
            " [ 0.         -0.        ]]\n",
            "gradients[ 1 ]= [ 0.01238043 -0.01632863]\n",
            "gradients[ 2 ]= [[ 0.06375708 -0.01882739]\n",
            " [ 0.11276554 -0.03329954]]\n",
            "gradients[ 3 ]= [ 0.14811596 -0.04373848]\n",
            "weights after gradient propagation in this step\n",
            "weights[ 0 ]= [[ 0.09257174  0.3097972 ]\n",
            " [-0.4         0.2       ]]\n",
            "weights[ 1 ]= [-0.40619022  0.8081643 ]\n",
            "weights[ 2 ]= [[ 0.36812147  0.20941369]\n",
            " [-0.45638278  0.7166498 ]]\n",
            "weights[ 3 ]= [0.725942   0.22186925]\n",
            "MSE with the new weights: 0.21819769800975905\n",
            "Learning rate =  0.5\n",
            " ############ Step = 1 ############\n",
            "1) ===BEFORE using any GRADIENT:===\n",
            "Network input [x1, x2]:\n",
            " [[1.2 0. ]]\n",
            "Network output [out_o1, out_o2]:\n",
            " [[0.6304009 0.7025409]]\n",
            "targets:\n",
            " [[0.  0.9]]\n",
            "weights at the beginning of this step\n",
            "weights[ 0 ]= [[ 0.09257174  0.3097972 ]\n",
            " [-0.4         0.2       ]]\n",
            "weights[ 1 ]= [-0.40619022  0.8081643 ]\n",
            "weights[ 2 ]= [[ 0.36812147  0.20941369]\n",
            " [-0.45638278  0.7166498 ]]\n",
            "weights[ 3 ]= [0.725942   0.22186925]\n",
            "MSE with the initial weights: 0.21819769800975905\n",
            "2) ===After applying GRADIENT:===\n",
            "------------- Results for step (iteration) = 1\n",
            "Gradients g to be used in new_weights = current_weights - learning_rate*g\n",
            "gradients[ 0 ]= [[ 0.01333611 -0.02084488]\n",
            " [ 0.         -0.        ]]\n",
            "gradients[ 1 ]= [ 0.01111343 -0.01737073]\n",
            "gradients[ 2 ]= [[ 0.06268202 -0.0176098 ]\n",
            " [ 0.11235394 -0.03156456]]\n",
            "gradients[ 3 ]= [ 0.14688064 -0.04126445]\n",
            "weights after gradient propagation in this step\n",
            "weights[ 0 ]= [[ 0.08590369  0.32021964]\n",
            " [-0.4         0.2       ]]\n",
            "weights[ 1 ]= [-0.41174692  0.81684965]\n",
            "weights[ 2 ]= [[ 0.33678046  0.2182186 ]\n",
            " [-0.5125598   0.73243207]]\n",
            "weights[ 3 ]= [0.6525017  0.24250147]\n",
            "MSE with the new weights: 0.1973207121255704\n",
            "Learning rate =  0.5\n",
            " ############ Step = 2 ############\n",
            "1) ===BEFORE using any GRADIENT:===\n",
            "Network input [x1, x2]:\n",
            " [[1.2 0. ]]\n",
            "Network output [out_o1, out_o2]:\n",
            " [[0.59895   0.7105263]]\n",
            "targets:\n",
            " [[0.  0.9]]\n",
            "weights at the beginning of this step\n",
            "weights[ 0 ]= [[ 0.08590369  0.32021964]\n",
            " [-0.4         0.2       ]]\n",
            "weights[ 1 ]= [-0.41174692  0.81684965]\n",
            "weights[ 2 ]= [[ 0.33678046  0.2182186 ]\n",
            " [-0.5125598   0.73243207]]\n",
            "weights[ 3 ]= [0.6525017  0.24250147]\n",
            "MSE with the initial weights: 0.1973207121255704\n",
            "2) ===After applying GRADIENT:===\n",
            "------------- Results for step (iteration) = 2\n",
            "Gradients g to be used in new_weights = current_weights - learning_rate*g\n",
            "gradients[ 0 ]= [[ 0.01170387 -0.02182248]\n",
            " [ 0.         -0.        ]]\n",
            "gradients[ 1 ]= [ 0.00975323 -0.0181854 ]\n",
            "gradients[ 2 ]= [[ 0.06092182 -0.0165018 ]\n",
            " [ 0.11059855 -0.02995766]]\n",
            "gradients[ 3 ]= [ 0.14387313 -0.0389707 ]\n",
            "weights after gradient propagation in this step\n",
            "weights[ 0 ]= [[ 0.08005175  0.3311309 ]\n",
            " [-0.4         0.2       ]]\n",
            "weights[ 1 ]= [-0.41662353  0.82594234]\n",
            "weights[ 2 ]= [[ 0.30631953  0.2264695 ]\n",
            " [-0.56785905  0.7474109 ]]\n",
            "weights[ 3 ]= [0.58056515 0.26198682]\n",
            "MSE with the new weights: 0.1774417708679269\n",
            "\n",
            " ===AFTER executing all GRADIENT descent steps===\n",
            "final weights[ 0 ]= [[ 0.08005175  0.3311309 ]\n",
            " [-0.4         0.2       ]]\n",
            "final weights[ 1 ]= [-0.41662353  0.82594234]\n",
            "final weights[ 2 ]= [[ 0.30631953  0.2264695 ]\n",
            " [-0.56785905  0.7474109 ]]\n",
            "final weights[ 3 ]= [0.58056515 0.26198682]\n",
            "outputs:\n",
            " [[0.567248  0.7180295]]\n",
            "targets:\n",
            " [[0.  0.9]]\n",
            "Final MSE =  0.1774417708679269\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras import backend as k\n",
        "from keras import losses\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#Initialize all weights to match the values adopted in the example at\n",
        "#https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/ \n",
        "#Initialise kernel (weights matrix) to required value\n",
        "def kernel_init_layer1(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (2, 2) \n",
        "    # and create a fixed array with this dimension for layer 1\n",
        "    kernel = np.array([[0.1, 0.3],[-0.4, 0.2]])\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (weights matrix) to required value\n",
        "def kernel_init_layer2(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (2, 2)\n",
        "    # and create a fixed array with this dimension for layer 2\n",
        "    kernel = np.array([[0.4, 0.2], [-0.4, 0.7]])\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (bias vector) to required value\n",
        "def kernel_init_bias1(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (1, 2)\n",
        "    # and create a fixed array with this dimension for the \n",
        "    # bias vector of layer 1\n",
        "    kernel = np.array([-0.4, 0.8]) #both neurons with the same value\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (bias vector) to required value\n",
        "def kernel_init_bias2(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (1, 2)\n",
        "    # and create a fixed array with this dimension for the \n",
        "    # bias vector of layer 2\n",
        "    kernel = np.array([0.8, 0.2]) #both neurons with the same value\n",
        "    return kernel \n",
        "\n",
        "# from https://stackoverflow.com/questions/66221788/tf-gradients-is-not-supported-when-eager-execution-is-enabled-use-tf-gradientta\n",
        "# TF 2 does not use \"eager\" execution, so disable it:\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "#Define the neural network model with dense layers. Syntax:\n",
        "#https://keras.io/api/layers/core_layers/dense/\n",
        "#The sigmoid activation function in Keras is the standard logistic function 1/(1+exp(-x)).\n",
        "#https://keras.io/api/layers/activations/\n",
        "model = Sequential()\n",
        "model.add(Dense(2, input_dim=2, use_bias=True,  bias_initializer=kernel_init_bias1,\n",
        "        kernel_initializer=kernel_init_layer1, activation='sigmoid'))\n",
        "model.add(Dense(2, use_bias=True,  bias_initializer=kernel_init_bias2, \n",
        "        kernel_initializer=kernel_init_layer2, activation='sigmoid'))\n",
        "model.summary() # display the architecture\n",
        "\n",
        "# We are not going to use a Keras optimizer. Define a learning rate:\n",
        "learning_rate = 0.5  #you can change to 0.5 or any other reasonable value\n",
        "\n",
        "#now compile the model, informing loss and performanc metrics that\n",
        "#should be computed along the training\n",
        "model.compile(loss='mse', metrics=['accuracy'])\n",
        "\n",
        "# Begin TensorFlow\n",
        "sess = tf.compat.v1.InteractiveSession()\n",
        "sess.run(tf.compat.v1.initialize_all_variables())\n",
        "\n",
        "model.optimizer.lr = learning_rate\n",
        "\n",
        "print(\"===START TRAINING THE NETWORK:===\")\n",
        "steps = 3 # steps of gradient descent\n",
        "for s in range(steps):\n",
        "    print('Learning rate = ', k.get_value(model.optimizer.lr))\n",
        "    print(\" ############ Step = \" + str(s) + \" ############\")\n",
        "    print(\"1) ===BEFORE using any GRADIENT:===\")\n",
        "    #define input and target vectors, and also inform it to the loss function object\n",
        "    #in this code we will keep the same inputs and targets along the steps, but the inputs and\n",
        "    #targets can change when we go, for instance, through the training set\n",
        "    inputs = np.array([[1.2,0]]) #define the input for the current iteration (step)\n",
        "    outputs = model.predict(inputs) #forward pass\n",
        "    #define the target vector for the current iteration (step)\n",
        "    targets = np.array([[0, 0.9]]) #notice that a sigmoid can output within range [0, 1]\n",
        "    mse = mean_squared_error(targets, outputs) #calculate MSE\n",
        "    #initialize loss object to be later incorporated to the gradients object that\n",
        "    #enables the calculation of the symbolic gradients\n",
        "    loss = losses.mean_squared_error(targets, model.output)\n",
        "    #  ===== Obtain symbolic gradient to calculate numerical gradients =====\n",
        "    gradients = k.gradients(loss, model.trainable_weights) #inform loss and weights\n",
        "    if False: #enable with True in case you want to see the objects\n",
        "        print(\"List of tensors representing the symbolic gradients:\")\n",
        "        for i in range(len(gradients)):\n",
        "            print('symbolic gradient[',i,']=',gradients[i])\n",
        "    print('Network input [x1, x2]:\\n', inputs)\n",
        "    print('Network output [out_o1, out_o2]:\\n', outputs)\n",
        "    print(\"targets:\\n\", targets)\n",
        "    #show weights at the beginning of iteration s\n",
        "    print('weights at the beginning of this step')\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        #note that model.trainable_weights[i] is an object of\n",
        "        # <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable\n",
        "        #therefore (see e.g. https://stackoverflow.com/questions/33679382/how-do-i-get-the-current-value-of-a-variable )\n",
        "        #you need to obtain its value via a TF session:\n",
        "        print('weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "    print(\"MSE with the initial weights:\", mse)\n",
        "\n",
        "    print(\"2) ===After applying GRADIENT:===\")\n",
        "    print(\"------------- Results for step (iteration) =\", s)\n",
        "    # ===== Calculate numerical gradient from symbolic gradients =====\n",
        "    # evaluated_gradients is a list, show its contents:\n",
        "    evaluated_gradients = sess.run(gradients, feed_dict={model.input: inputs})\n",
        "    print('Gradients g to be used in new_weights = current_weights - learning_rate*g')\n",
        "    for i in range(len(evaluated_gradients)):\n",
        "        print('gradients[',i,']=',evaluated_gradients[i])\n",
        "\n",
        "    # Apply (\"step down\") the gradient for each layer, subtracting the gradients\n",
        "    # from current weights scaled by the learning rate:\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        sess.run(tf.compat.v1.assign_sub(model.trainable_weights[i], learning_rate*evaluated_gradients[i]))\n",
        "\n",
        "    #show weights after gradient propagation of iteration s\n",
        "    print('weights after gradient propagation in this step')\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        print('weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "\n",
        "    # print the MSE with new weights:\n",
        "    outputs = model.predict(inputs)\n",
        "    mse = mean_squared_error(targets, outputs)\n",
        "    print(\"MSE with the new weights:\", mse)\n",
        "\n",
        "#Collect and show final results\n",
        "final_outputs = model.predict(inputs)\n",
        "final_mse = mean_squared_error(targets, final_outputs)\n",
        "\n",
        "print(\"\\n ===AFTER executing all GRADIENT descent steps===\")\n",
        "#show weights at the end of training\n",
        "for i in range(len(model.trainable_weights)):\n",
        "    #note that model.trainable_weights[i] is an object of\n",
        "    # <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable\n",
        "    #therefore (see e.g. https://stackoverflow.com/questions/33679382/how-do-i-get-the-current-value-of-a-variable )\n",
        "    #you need to obtain its value via a TF session:\n",
        "    print('final weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "print(\"outputs:\\n\", final_outputs)\n",
        "print(\"targets:\\n\", targets)\n",
        "print(\"Final MSE = \", final_mse)"
      ]
    }
  ]
}